<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Converted Markdown</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/vs2015.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.6;
            color: #e0e0e0;
            background-color: #1e1e1e;
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
        }
        .tree {
            margin-left: 20px;
        }
        .folder {
            font-weight: bold;
            margin-top: 10px;
            margin-bottom: 5px;
        }
        pre {
            background-color: #1e1e1e;
            border-radius: 4px;
            padding: 16px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            font-size: 14px;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 1em;
        }
        th, td {
            border: 1px solid #4a4a4a;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #2d2d2d;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        a {
            color: #569cd6;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #e0e0e0;
        }
    </style>
</head>
<body>
    <h1>Transformers Notes</h1>
    <div class="tree">
<div class="folder">Transformers</div>
<div class="tree">
    <div class="folder">Transformers - 3Blue1Brown</div>
    <div class="tree">
        <a href="Transformers/Transformers - 3Blue1Brown/DL5 | Transformers (how LLMs work) explained visually.html">DL5 | Transformers (how LLMs work) explained visually</a><br>
        <a href="Transformers/Transformers - 3Blue1Brown/DL6 | Attention in transformers, visually explained.html">DL6 | Attention in transformers, visually explained</a><br>
        <a href="Transformers/Transformers - 3Blue1Brown/DL7 | How might LLMs store facts.html">DL7 | How might LLMs store facts</a><br>
    </div>
    <div class="folder">Transformers & Implementation - Umar Jamil</div>
    <div class="tree">
        <a href="Transformers/Transformers & Implementation - Umar Jamil/Attention is all you need (Transformer) - Model explanation (including math), Inference and Training.html">Attention is all you need (Transformer) - Model explanation (including math), Inference and Training</a><br>
        <a href="Transformers/Transformers & Implementation - Umar Jamil/Coding a Transformer from scratch on PyTorch, with full explanation, training and inference..html">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.</a><br>
        <a href="Transformers/Transformers & Implementation - Umar Jamil/model.py.html">model.py</a><br>
    </div>
    <div class="folder">Flash Attention from Scratch - Umar Jamil</div>
    <div class="tree">
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/1 - Safe Softmax.html">1 - Safe Softmax</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/3 - Forward Pass.html">3 - Forward Pass</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/4 - Autograd.html">4 - Autograd</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/5 - Softmax Jacobian.html">5 - Softmax Jacobian</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/6 - Backward Pass.html">6 - Backward Pass</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/Linear Algebra Intuition.html">Linear Algebra Intuition</a><br>
    </div>
</div>
<div class="folder">Triton</div>
<div class="tree">
    <a href="Triton/Triton Basics.html">Triton Basics</a><br>
    <a href="Triton/Triton-Puzzles.html">Triton-Puzzles</a><br>
</div>
    </div>

    <h3>Resources Used</h3>
    <ul>
        <li>Transformers (last 3 videos from this playlist): <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Networks Playlist</a></li>
        <li>Transformers & Implementation - Umar Jamil:
            <ul>
                <li>Attention Model explanation (including math), Inference and Training: <a href="https://www.youtube.com/watch?v=bCz4OMemCcA">Video</a></li>
                <li>Coding a Transformer from scratch on PyTorch: <a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Video</a></li>
            </ul>
        </li>
        <li>Flash attention from scratch: <a href="https://www.youtube.com/watch?v=zy8ChVd_oTM">Video</a></li>
        <li>These notes include ideas from other sites as well, so this list isn't exhaustive</li>
    </ul>
</body>
</html>
