<!DOCTYPE html>
<html>
<head>
    <title>Transformers Notes</title>
    <style>
        .tree { margin-left: 20px; }
        .folder { font-weight: bold; }
        body { font-family: Arial, sans-serif; }
        a { text-decoration: none; color: #0366d6; }
        a:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <h1>Transformers Notes</h1>
    <div class="tree">
<div class="folder">Transformers</div>
<div class="tree">
    <div class="folder">Transformers - 3Blue1Brown</div>
    <div class="tree">
        <a href="Transformers/Transformers - 3Blue1Brown/DL5 | Transformers (how LLMs work) explained visually.html">DL5 | Transformers (how LLMs work) explained visually</a><br>
        <a href="Transformers/Transformers - 3Blue1Brown/DL6 | Attention in transformers, visually explained.html">DL6 | Attention in transformers, visually explained</a><br>
        <a href="Transformers/Transformers - 3Blue1Brown/DL7 | How might LLMs store facts.html">DL7 | How might LLMs store facts</a><br>
    </div>
    <div class="folder">Transformers & Implementation - Umar Jamil</div>
    <div class="tree">
        <a href="Transformers/Transformers & Implementation - Umar Jamil/Attention is all you need (Transformer) - Model explanation (including math), Inference and Training.html">Attention is all you need (Transformer) - Model explanation (including math), Inference and Training</a><br>
        <a href="Transformers/Transformers & Implementation - Umar Jamil/Coding a Transformer from scratch on PyTorch, with full explanation, training and inference..html">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.</a><br>
        <a href="Transformers/Transformers & Implementation - Umar Jamil/model.py.html">model.py</a><br>
    </div>
    <div class="folder">Flash Attention from Scratch - Umar Jamil</div>
    <div class="tree">
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/1 - Safe Softmax.html">1 - Safe Softmax</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/3 - Forward Pass.html">3 - Forward Pass</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/4 - Autograd.html">4 - Autograd</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/5 - Softmax Jacobian.html">5 - Softmax Jacobian</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/6 - Backward Pass.html">6 - Backward Pass</a><br>
        <a href="Transformers/Flash Attention from Scratch - Umar Jamil/Linear Algebra Intuition.html">Linear Algebra Intuition</a><br>
    </div>
</div>
<div class="folder">Triton</div>
<div class="tree">
    <a href="Triton/Triton Basics.html">Triton Basics</a><br>
    <a href="Triton/Triton-Puzzles.html">Triton-Puzzles</a><br>
</div>
    </div>

    <h3>Resources Used</h3>
    <ul>
        <li>Transformers (last 3 videos from this playlist): <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Networks Playlist</a></li>
        <li>Transformers & Implementation - Umar Jamil:
            <ul>
                <li>Attention Model explanation (including math), Inference and Training: <a href="https://www.youtube.com/watch?v=bCz4OMemCcA">Video</a></li>
                <li>Coding a Transformer from scratch on PyTorch: <a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Video</a></li>
            </ul>
        </li>
        <li>Flash attention from scratch: <a href="https://www.youtube.com/watch?v=zy8ChVd_oTM">Video</a></li>
        <li>These notes include ideas from other sites as well, so this list isn't exhaustive</li>
    </ul>
</body>
</html>
